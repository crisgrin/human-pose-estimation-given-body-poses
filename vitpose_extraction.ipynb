{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162710,"status":"ok","timestamp":1657022510068,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"qQ2vdKEKqOHI","outputId":"c0e843c2-b41c-490c-e915-311719057d09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ViTPose'...\n","remote: Enumerating objects: 1737, done.\u001b[K\n","remote: Counting objects: 100% (1726/1726), done.\u001b[K\n","remote: Compressing objects: 100% (894/894), done.\u001b[K\n","remote: Total 1737 (delta 814), reused 1705 (delta 802), pack-reused 11\u001b[K\n","Receiving objects: 100% (1737/1737), 10.66 MiB | 5.55 MiB/s, done.\n","Resolving deltas: 100% (817/817), done.\n","Using pip 22.1.2 from /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/pip (python 3.9)\n","Obtaining file:///Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/ViTPose\n","  Preparing metadata (setup.py) ... \u001b[?25l  Running command python setup.py egg_info\n","  running egg_info\n","  creating /private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info\n","  writing /private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/PKG-INFO\n","  writing dependency_links to /private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/dependency_links.txt\n","  writing requirements to /private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/requires.txt\n","  writing top-level names to /private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/top_level.txt\n","  writing manifest file '/private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/SOURCES.txt'\n","  reading manifest file '/private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/SOURCES.txt'\n","  reading manifest template 'MANIFEST.in'\n","  warning: no files found matching 'mmpose/.mim/model-index.yml'\n","  warning: no files found matching '*.py' under directory 'mmpose/.mim/configs'\n","  warning: no files found matching '*.yml' under directory 'mmpose/.mim/configs'\n","  warning: no files found matching '*.py' under directory 'mmpose/.mim/tools'\n","  warning: no files found matching '*.sh' under directory 'mmpose/.mim/tools'\n","  warning: no files found matching '*.py' under directory 'mmpose/.mim/demo'\n","  adding license file 'LICENSE'\n","  writing manifest file '/private/var/folders/mj/_zlzn07d7xnd1bcr0_w6w2980000gn/T/pip-pip-egg-info-juodrx_m/mmpose.egg-info/SOURCES.txt'\n","\u001b[?25hdone\n","Requirement already satisfied: chumpy in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (0.70)\n","Requirement already satisfied: json_tricks in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (3.15.5)\n","Requirement already satisfied: matplotlib in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (3.5.2)\n","Requirement already satisfied: munkres in /Users/cristianverdecchia/.local/lib/python3.9/site-packages/munkres-1.1.4-py3.9.egg (from mmpose==0.24.0) (1.1.4)\n","Requirement already satisfied: numpy in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (1.23.1)\n","Requirement already satisfied: opencv-python in /Users/cristianverdecchia/.local/lib/python3.9/site-packages/opencv_python-4.5.5.64-py3.9-macosx-11.1-arm64.egg (from mmpose==0.24.0) (4.5.5.64)\n","Requirement already satisfied: pillow in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (9.2.0)\n","Requirement already satisfied: scipy in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (1.9.0)\n","Requirement already satisfied: torchvision in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (0.13.1)\n","Requirement already satisfied: xtcocotools>=1.8 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from mmpose==0.24.0) (1.12)\n","Requirement already satisfied: setuptools>=18.0 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from xtcocotools>=1.8->mmpose==0.24.0) (61.2.0)\n","Requirement already satisfied: cython>=0.27.3 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from xtcocotools>=1.8->mmpose==0.24.0) (0.29.32)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from matplotlib->mmpose==0.24.0) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from matplotlib->mmpose==0.24.0) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from matplotlib->mmpose==0.24.0) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from matplotlib->mmpose==0.24.0) (21.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from matplotlib->mmpose==0.24.0) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from matplotlib->mmpose==0.24.0) (4.34.4)\n","Requirement already satisfied: six>=1.11.0 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from chumpy->mmpose==0.24.0) (1.15.0)\n","Requirement already satisfied: torch in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from torchvision->mmpose==0.24.0) (1.12.1)\n","Requirement already satisfied: typing-extensions in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from torchvision->mmpose==0.24.0) (4.3.0)\n","Requirement already satisfied: requests in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from torchvision->mmpose==0.24.0) (2.28.1)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from requests->torchvision->mmpose==0.24.0) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from requests->torchvision->mmpose==0.24.0) (2022.6.15)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from requests->torchvision->mmpose==0.24.0) (1.26.11)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages (from requests->torchvision->mmpose==0.24.0) (2.1.0)\n","Installing collected packages: mmpose\n","  Attempting uninstall: mmpose\n","    Found existing installation: mmpose 0.28.0\n","    Uninstalling mmpose-0.28.0:\n","      Removing file or directory /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/mmpose.egg-link\n","      Removing pth entries from /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/easy-install.pth:\n","      Removing entry: /Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/mmpose\n","      Successfully uninstalled mmpose-0.28.0\n","  Running setup.py develop for mmpose\n","    Running command python setup.py develop\n","    running develop\n","    /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n","      warnings.warn(\n","    /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n","      warnings.warn(\n","    running egg_info\n","    creating mmpose.egg-info\n","    writing mmpose.egg-info/PKG-INFO\n","    writing dependency_links to mmpose.egg-info/dependency_links.txt\n","    writing requirements to mmpose.egg-info/requires.txt\n","    writing top-level names to mmpose.egg-info/top_level.txt\n","    writing manifest file 'mmpose.egg-info/SOURCES.txt'\n","    reading manifest file 'mmpose.egg-info/SOURCES.txt'\n","    reading manifest template 'MANIFEST.in'\n","    adding license file 'LICENSE'\n","    writing manifest file 'mmpose.egg-info/SOURCES.txt'\n","    running build_ext\n","    Creating /Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/mmpose.egg-link (link to .)\n","    Adding mmpose 0.24.0 to easy-install.pth file\n","\n","    Installed /Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose\n","Successfully installed mmpose\n"]}],"source":["#import torch\n","#print(torch.__version__)\n","# !pip uninstall torch -y\n","# !pip uninstall torchvision -y\n","# !pip uninstall torchaudio -y\n","\n","# #!nvcc --version\n","# !pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0\n","\n","# !pip install timm==0.4.9 einops\n","\n","# !pip install apex\n","\n","# !pip install mmdet\n","\n","# !pip install matplotlib\n","\n","# !pip install mmcv-full==1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n","\n","!git clone https://github.com/ViTAE-Transformer/ViTPose.git\n","!pip install -v -e ./ViTPose"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12452,"status":"ok","timestamp":1657022522439,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"MCPrKDNcBhNz","outputId":"ec281372-ce74-4a6d-fbdf-c62ae1617dd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: wget\n"]}],"source":["# Results from this repo on MS COCO val set (multi-task training)\n","# MODEL B\n","!wget -O model_b.pth https://fd0rfg.sn.files.1drv.com/y4mjxZwPM-eGNFmXLsf91PehujeQMrrF_3u34fFj5aaZyJVLka85mkdL8CXELnZUv5dINCzwWk_gtuxjs5DSluasUcio_U1DRqNAnMFeaACjUuYq65nIV7UBzeutRE1OE2HYfkD79eXHaD-VR0r8txxAQot2TQOnafJgBfPQSxgMyak93Gt8_V7fr4WZFJbFy_fasyBRBl1OYh_jOr--167fQ"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"elapsed":23,"status":"error","timestamp":1657022522441,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"ikVMuTVsFJD3","outputId":"98ed82d2-1b20-4557-c3d7-a2dc70ce6dfa"},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'container_abcs' from 'torch._six' (/Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/torch/_six.py)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m/Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapis\u001b[39;00m \u001b[39mimport\u001b[39;00m (inference_bottom_up_pose_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                          inference_top_down_pose_model, init_pose_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                          process_mmdet_results, vis_pose_result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetInfo\n","File \u001b[0;32m~/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose/mmpose/apis/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minference\u001b[39;00m \u001b[39mimport\u001b[39;00m (inference_bottom_up_pose_model,\n\u001b[1;32m      3\u001b[0m                         inference_top_down_pose_model, init_pose_model,\n\u001b[1;32m      4\u001b[0m                         process_mmdet_results, vis_pose_result)\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minference_3d\u001b[39;00m \u001b[39mimport\u001b[39;00m (extract_pose_sequence, inference_interhand_3d_model,\n\u001b[1;32m      6\u001b[0m                            inference_mesh_model, inference_pose_lifter_model,\n\u001b[1;32m      7\u001b[0m                            vis_3d_mesh_result, vis_3d_pose_result)\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minference_tracking\u001b[39;00m \u001b[39mimport\u001b[39;00m get_track_id, vis_pose_tracking_result\n","File \u001b[0;32m~/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose/mmpose/apis/inference.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_info\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetInfo\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m \u001b[39mimport\u001b[39;00m Compose\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m build_posenet\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmpose\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mimport\u001b[39;00m OutputHook\n\u001b[1;32m     18\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mKMP_DUPLICATE_LIB_OK\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTRUE\u001b[39m\u001b[39m'\u001b[39m\n","File \u001b[0;32m~/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose/mmpose/models/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackbones\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m (BACKBONES, HEADS, LOSSES, MESH_MODELS, NECKS, POSENETS,\n\u001b[1;32m      4\u001b[0m                       build_backbone, build_head, build_loss, build_mesh_model,\n\u001b[1;32m      5\u001b[0m                       build_neck, build_posenet)\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdetectors\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n","File \u001b[0;32m~/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose/mmpose/models/backbones/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvipnas_mbv3\u001b[39;00m \u001b[39mimport\u001b[39;00m ViPNAS_MobileNetV3\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvipnas_resnet\u001b[39;00m \u001b[39mimport\u001b[39;00m ViPNAS_ResNet\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvit\u001b[39;00m \u001b[39mimport\u001b[39;00m ViT\n\u001b[1;32m     29\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     30\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mAlexNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHourglassNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHourglassAENet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHRNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMobileNetV2\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMobileNetV3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRegNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mResNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mResNetV1d\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mResNeXt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSCNet\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLiteHRNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mV2VNet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHRFormer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mViT\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     35\u001b[0m ]\n","File \u001b[0;32m~/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose/mmpose/models/backbones/vit.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcheckpoint\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtimm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m drop_path, to_2tuple, trunc_normal_\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m BACKBONES\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase_backbone\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseBackbone\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/timm-0.1.20-py3.9.egg/timm/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m create_model, list_models, is_model, list_modules, model_entrypoint\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/timm-0.1.20-py3.9.egg/timm/models/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minception_v4\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minception_resnet_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdensenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/timm-0.1.20-py3.9.egg/timm/models/inception_v4.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m register_model\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhelpers\u001b[39;00m \u001b[39mimport\u001b[39;00m load_pretrained\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m SelectAdaptivePool2d\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtimm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n\u001b[1;32m     14\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mInceptionV4\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/timm-0.1.20-py3.9.egg/timm/models/layers/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpadding\u001b[39;00m \u001b[39mimport\u001b[39;00m get_padding\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpool2d_same\u001b[39;00m \u001b[39mimport\u001b[39;00m AvgPool2dSame\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconv2d_same\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2dSame\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconv_bn_act\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvBnAct\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/timm-0.1.20-py3.9.egg/timm/models/layers/pool2d_same.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union, List, Tuple, Optional\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhelpers\u001b[39;00m \u001b[39mimport\u001b[39;00m tup_pair\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpadding\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_same, get_padding_value\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mavg_pool2d_same\u001b[39m(x, kernel_size: List[\u001b[39mint\u001b[39m], stride: List[\u001b[39mint\u001b[39m], padding: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m     16\u001b[0m                     ceil_mode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, count_include_pad: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     17\u001b[0m     \u001b[39m# FIXME how to deal with count_include_pad vs not for external padding?\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.9/site-packages/timm-0.1.20-py3.9.egg/timm/models/layers/helpers.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\" Layer/Module Helpers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39mHacked together by Ross Wightman\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m repeat\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_six\u001b[39;00m \u001b[39mimport\u001b[39;00m container_abcs\n\u001b[1;32m      9\u001b[0m \u001b[39m# From PyTorch internals\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ntuple\u001b[39m(n):\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'container_abcs' from 'torch._six' (/Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/torch/_six.py)"]}],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import copy\n","import json\n","import os\n","import multiprocessing\n","\n","import numpy as np\n","from mmpose.apis import (inference_bottom_up_pose_model,\n","                         inference_top_down_pose_model, init_pose_model,\n","                         process_mmdet_results, vis_pose_result)\n","from mmpose.datasets import DatasetInfo"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1657022522439,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"d5fKfc8jFhtO"},"outputs":[],"source":["def init_model():\n","  return init_pose_model(\n","      './ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/'\n","      'coco/ViTPose_base_coco_256x192.py',\n","      'model_b.pth',\n","      device='cpu')\n","\n","def init_dataset_info(pose_model):\n","  dataset_info = DatasetInfo(pose_model.cfg.data['test'].get(\n","      'dataset_info', None))\n","    \n","\n","def vis_top_down(pose_model, dataset_info, person_result, image_path):\n","  # # test a single image, with a list of bboxes.\n","  pose_results = get_pose(\n","      pose_model, dataset_info, person_result, image_path)\n","  \n","  # show the results\n","  return vis_pose_result(\n","      pose_model, image_path, pose_results, dataset_info=dataset_info)\n","\n","def get_pose(pose_model, dataset_info, person_result, image_path):\n","  # # test a single image, with a list of bboxes.\n","  pose_results, _ = inference_top_down_pose_model(\n","      pose_model,\n","      image_path,\n","      person_result,\n","      format='xyxy',\n","      dataset_info=dataset_info)\n","  return pose_results"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1657022522439,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"ZeJF-8MVKbJ0"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m\"\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import files\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1657022522440,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"WiD4bzZN6HVX"},"outputs":[{"ename":"NameError","evalue":"name 'init_pose_model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../annotation/annotation_image_info.json\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   annotations \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pose_model \u001b[39m=\u001b[39m init_model()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset_info \u001b[39m=\u001b[39m init_dataset_info(pose_model)\n","\u001b[1;32m/Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb Cell 6\u001b[0m in \u001b[0;36minit_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_model\u001b[39m():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m init_pose_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m./ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcoco/ViTPose_base_coco_256x192.py\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mmodel_b.pth\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/vitpose_extraction.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m       device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'init_pose_model' is not defined"]}],"source":["with open('../annotation/annotation_image_info.json') as f:\n","  annotations = json.load(f)\n","\n","pose_model = init_model()\n","dataset_info = init_dataset_info(pose_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1657022522440,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"1rMlvMA9h_n5"},"outputs":[],"source":["class PostureLink():\n","  def __init__(self, posture_id, image_detection):\n","    self.posture_id = posture_id\n","    self.image_detection = image_detection\n","\n","  def toJSON(self):\n","    return json.dumps(self, default=lambda o: o.__dict__,\n","                     sort_keys=True, indent=4)\n","\n","\n","class ImageDetectionLink():\n","  def __init__(self, image_name, relation_ids, person_1, person_2):\n","    self.image_name = image_name  \n","    self.relation_ids = relation_ids\n","    self.person_1 = person_1\n","    self.person_2 = person_2\n","  # def add_pose(self, pose):\n","  #   self.people_poses.append(pose)\n","\n","  def toJSON(self):\n","    return json.dumps(self, default=lambda o: o.__dict__,\n","                     sort_keys=True, indent=4)\n","    \n","\n","class PersonDetection():\n","  def __init__(self, left_eye, right_eye, left_ear, right_ear,  left_shoulder, \n","               right_shoulder, left_elbow, right_elbow, left_wrist, right_wrist, \n","               left_hip, right_hip, left_knee, right_knee, left_ankle, right_ankle):\n","\n","    self.left_eye = left_eye.tolist()\n","    self.right_eye = right_eye.tolist()\n","    self.left_ear = left_ear.tolist()\n","    self.right_ear = right_ear.tolist()\n","    self.left_shoulder = left_shoulder.tolist()\n","    self.right_shoulder = right_shoulder.tolist()\n","    self.left_elbow = left_elbow.tolist()\n","    self.right_elbow = right_elbow.tolist()\n","    self.left_wrist = left_wrist.tolist()\n","    self.right_wrist = right_wrist.tolist()\n","    self.left_hip = left_hip.tolist()\n","    self.right_hip = right_hip.tolist()\n","    self.left_knee = left_knee.tolist()\n","    self.right_knee = right_knee.tolist()\n","    self.left_ankle = left_ankle.tolist()\n","    self.right_ankle = right_ankle.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1657022522440,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"oTZ-0Xj9yUJa"},"outputs":[],"source":["#Creates a new detection object (check the config for the meaning of the indexes)\n","def create_detection(detection):\n","  return PersonDetection(\n","      detection[0],\n","      detection[1],\n","      detection[2],\n","      detection[3],\n","      detection[4],\n","      detection[5],\n","      detection[6],\n","      detection[7],\n","      detection[8],\n","      detection[9],\n","      detection[10],\n","      detection[11],\n","      detection[12],\n","      detection[13],\n","      detection[14],\n","      detection[15]\n","      )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1657022522440,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"oEPEUAvf_FgI"},"outputs":[],"source":["directory = '/content/drive/MyDrive/AdvancedComputerVision/image/'\n","\n","#list_of_files_from_folder = [s.lstrip(\"0\") for s in sorted(os.listdir(directory))]\n","\n","# Estimates poses and creates Detection objects for each pose\n","def detect_poses(file_annotations):\n","  boxes = []\n","\n","  file_name = file_annotations[1]['file_name']\n","  # Transform values inside to int\n","  bblist = list(map(int, file_annotations[1]['pose']))\n","\n","  #  image_id = int(image_name[:-4])\n","  image_id = int(file_name)\n","\n","  file_name = file_name + \".jpg\"\n","  for x in range(5 - len(file_name[:-4])):\n","    file_name = \"0\" + file_name\n","\n","  # Create an object with the file and the list of poses\n","  for annotation in annotations:\n","      if annotation['id'] == image_id:\n","          # The -1 is required for the index since the indexes represent the \n","          # relations between people ex: person 1 and person 2, there's no person 0\n","          boxes.append({'bbox': annotation['bbox'][bblist[0]-1]})\n","          boxes.append({'bbox': annotation['bbox'][bblist[1]-1]})\n","          print(file_name+\" annotation ID: \"+str(image_id))\n","\n","  # Obtain poses for each bbox\n","  obtained_poses = get_pose(pose_model, dataset_info, boxes, directory+file_name)\n","  \n","  first_person = create_detection(obtained_poses[0]['keypoints'][:,:-1])\n","  second_person = create_detection(obtained_poses[1]['keypoints'][:,:-1])\n","\n","  image_detection = ImageDetectionLink(image_id, bblist, first_person, second_person)\n","  posture_link = {file_annotations[0] : image_detection}\n","  \n","  return posture_link"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1657022522440,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"vY2ZjF9iZE1r"},"outputs":[],"source":["# TODO CHANGE IT FOR COUPLE OF POSES INSTEAD OF FILES\n","# file_dict = {1:[], 2:[],3:[], 4:[], 5:[],6:[]}\n","# for file_name in relationships.keys():\n","#   for people_link in relationships[file_name]:\n","#     if file_name not in file_dict[relationships[file_name][people_link]]:\n","#       file_dict[relationships[file_name][people_link]].append(str(file_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1657022522441,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"LlVCc1z-RBC8"},"outputs":[],"source":["with open('/content/drive/MyDrive/AdvancedComputerVision/annotation/relationship.json') as f:\n","  relationships = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":711,"status":"ok","timestamp":1656255843452,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"-TCLy5bJmsW-","outputId":"56128cdb-587e-4859-cf59-b625cea292d1"},"outputs":[],"source":["boxes = {1:{}, 2:{}, 3:{}, 4:{}, 5:{}, 6:{}}\n","\n","# Loop all the files and relationships\n","for file_id, dict_link in relationships.items():\n","    # Loop the relationships with labels\n","    for linking, label in dict_link.items():\n","      indexes = linking.split()\n","      # Check if the labels have already been added\n","      if file_id not in boxes[label]:\n","        boxes[label].update({file_id: [indexes]})\n","      else:\n","        boxes[label][file_id].append(indexes)\n","\n","print(boxes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kTckhkodosY"},"outputs":[],"source":["import random\n","from itertools import islice\n","\n","# # Creates a subset for a given label\n","# def get_label_data(index, amount):    \n","#   label = islice(\n","#       boxes[index].items(), amount)\n","#   return dict(label)\n","\n","# label_1 = get_label_data(1, 500)\n","# label_2 = get_label_data(2, 500)\n","# label_3 = get_label_data(3, 500)\n","# label_4 = get_label_data(4, 500)\n","# label_5 = get_label_data(5, 500)\n","# label_6 = get_label_data(6, 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEQlHsnIUP4i"},"outputs":[],"source":["# Saves the full annotations\n","\n","label_1 = dict(boxes[1].items())\n","label_2 = dict(boxes[2].items())\n","label_3 = dict(boxes[3].items())\n","label_4 = dict(boxes[4].items())\n","label_5 = dict(boxes[5].items())\n","label_6 = dict(boxes[6].items())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4hFDfNEgmX_"},"outputs":[],"source":["class LabelData():\n","  def __init__(self, label_1, label_2, label_3, label_4, label_5, label_6):\n","    self.label_1 = label_1\n","    self.label_2 = label_2\n","    self.label_3 = label_3\n","    self.label_4 = label_4\n","    self.label_5 = label_5\n","    self.label_6 = label_6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_eAnebphIHB"},"outputs":[],"source":["labels_to_save = LabelData(label_1, label_2, label_3, label_4, label_5, label_6)\n","\n","with open(\"/content/drive/MyDrive/AdvancedComputerVision/annotation/labels_with_files_s_500.json\", \"w\") as outfile:\n","    json.dump(labels_to_save, outfile, default=lambda o: o.__dict__ )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-h4hja4wBdc"},"outputs":[],"source":["import json\n","\n","with open('/content/drive/MyDrive/AdvancedComputerVision/annotation/labels_with_files_s_500.json') as f:\n","  file_annotations = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVOsV3Z1XOSb"},"outputs":[],"source":["label_dict = {1:{}, 2:{}, 3:{}, 4:{}, 5:{}, 6:{}}\n","index = 0\n","\n","# Loop through the labels\n","for label, value in enumerate(file_annotations.values()):\n","  # Loop each file\n","  for link in value.items():\n","    for pose in link[1]:\n","      # Add the file and the poses to the dict\n","      inner_dict = {'file_name': link[0], 'pose': pose}\n","      # Update the dict with file name and poses\n","      temp_dict = {index : inner_dict}\n","      label_dict[label+1].update(temp_dict)\n","      index = index + 1\n","\n","# for label, stuff in enumerate(label_dict.values()):\n","#   print(len(stuff))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1656258456904,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"9xjPjWvjVMuM","outputId":"1cb0464b-dee8-4548-b82e-31a025c1b86a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Images per label\n","1 5129\n","2 3733\n","3 1890\n","4 7308\n","5 673\n","6 2439\n","Poses per label\n","1 13808\n","2 8744\n","3 1910\n","4 22011\n","5 1041\n","6 13352\n"]}],"source":["print(\"Images per label\")\n","# Amount of images per label\n","for label, value in enumerate(file_annotations.values()):\n","  print(label+1, len(value))\n","\n","print(\"Poses per label\")\n","# Amount of poses per label\n","for label, value in enumerate(label_dict.values()):\n","  print(label+1, len(value.items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyNkf4ehT3X8"},"outputs":[],"source":["from itertools import islice\n","\n","# Transform the dict into chunks (does work but not properly)\n","# This has to be used just for tests purposes\n","# def chunks(data, SIZE=10000):\n","#     it = iter(data)\n","#     for i in range(0, len(data), SIZE):\n","#         yield {k:data[k] for k in islice(it, SIZE)}\n","# # Put the samples into a dict\n","# for item in chunks(label_dict[1], 10):\n","#   test = item\n","# print(len(test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":12,"status":"error","timestamp":1656255844069,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"},"user_tz":-120},"id":"SzTn1_eZVkW-","outputId":"a322ad0f-d957-4a49-f5e5-4e21dacd801e"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-69d2f4141e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_label_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'get_label_data' is not defined"]}],"source":["# def get_label_data_to_run(index, amount):    \n","#   label = islice(\n","#       label_dict[index].items(), amount)\n","#   return dict(label)\n","\n","# print(len(get_label_data(1, 500).items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yp62EZ2FwCMq"},"outputs":[],"source":["label_to_run = 5\n","\n","# label_dict_to_run = get_label_data_to_run(label_to_run, 500)\n","label_dict_to_run = label_dict[label_to_run]\n","print(len(label_dict_to_run))\n","\n","#Parallelise the process, so to compute the same for multiple images\n","multiprocessing.set_start_method('fork', force=True)\n","pool = multiprocessing.Pool(int(os.cpu_count()))\n","\n","# Uses all of the available labels\n","image_detection_list = pool.map(detect_poses, label_dict_to_run.items())\n","\n","# Uses only few examples to test the functionality\n","# image_detection_list = pool.map(detect_poses, test.items())\n","pool.close()\n","\n","\n","#{image_name: 'im.jpg', body_poses=[['leftknee'..., label=1]['right_knee'...,label=2]]}}\n","\n","print(\"Execution completed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRl9QJkmmGK6"},"outputs":[],"source":["final_dict = {}\n","\n","for image_detection in image_detection_list:\n","  image_id = list(image_detection.keys())[0]\n","  final_dict[image_id] = list(image_detection.values())[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kr9wCWtefbS6"},"outputs":[],"source":["with open(\"/content/drive/MyDrive/AdvancedComputerVision/posture_data/\"+str(label_to_run)+\"_full_bb.json\", \"w\") as outfile:\n","        json.dump( final_dict, outfile, default=lambda o: o.__dict__ )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYQ8OLD7Btl-"},"outputs":[],"source":["len(image_detection_list)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"vitpose_extraction.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('skeletoncnn')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"f2dbd656668c842d22c7cc804324ef75aa425d033a96c56443a55074cc19dd74"}}},"nbformat":4,"nbformat_minor":0}
