{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ2vdKEKqOHI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c579a21-b630-4cec-dd40-74843125ce79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 1.11.0+cu113\n","Uninstalling torch-1.11.0+cu113:\n","  Successfully uninstalled torch-1.11.0+cu113\n","Found existing installation: torchvision 0.12.0+cu113\n","Uninstalling torchvision-0.12.0+cu113:\n","  Successfully uninstalled torchvision-0.12.0+cu113\n","Found existing installation: torchaudio 0.11.0+cu113\n","Uninstalling torchaudio-0.11.0+cu113:\n","  Successfully uninstalled torchaudio-0.11.0+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████▏           | 522.7 MB 1.2 MB/s eta 0:04:13"]}],"source":["#import torch\n","#print(torch.__version__)\n","!pip uninstall torch -y\n","!pip uninstall torchvision -y\n","!pip uninstall torchaudio -y\n","\n","#!nvcc --version\n","!pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0\n","\n","!pip install timm==0.4.9 einops\n","\n","!pip install apex\n","\n","!pip install mmdet\n","\n","!pip install matplotlib\n","\n","!pip install mmcv-full==1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n","\n","!git clone https://github.com/ViTAE-Transformer/ViTPose.git\n","!pip install -v -e ./ViTPose"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCPrKDNcBhNz"},"outputs":[],"source":["# Results from this repo on MS COCO val set (multi-task training)\n","# MODEL B\n","!wget -O model_b.pth https://fd0rfg.sn.files.1drv.com/y4mjxZwPM-eGNFmXLsf91PehujeQMrrF_3u34fFj5aaZyJVLka85mkdL8CXELnZUv5dINCzwWk_gtuxjs5DSluasUcio_U1DRqNAnMFeaACjUuYq65nIV7UBzeutRE1OE2HYfkD79eXHaD-VR0r8txxAQot2TQOnafJgBfPQSxgMyak93Gt8_V7fr4WZFJbFy_fasyBRBl1OYh_jOr--167fQ"]},{"cell_type":"code","source":["!cd ViTPose && mkdir data\n","!cd ViTPose/data && mkdir coco\n","#!unzip annotations_trainval2017.zip -d ViTPose/data/coco"],"metadata":{"id":"Ezar4oHWRrvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import copy\n","import json\n","import os\n","\n","import numpy as np\n","from mmpose.apis import (inference_bottom_up_pose_model,\n","                         inference_top_down_pose_model, init_pose_model,\n","                         process_mmdet_results, vis_pose_result)\n","from mmpose.datasets import DatasetInfo"],"metadata":{"id":"ikVMuTVsFJD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_model():\n","  return init_pose_model(\n","      '/content/ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/'\n","      'coco/ViTPose_base_coco_256x192.py',\n","      'model_b.pth',\n","      device='cpu')\n","\n","def init_dataset_info(pose_model):\n","  dataset_info = DatasetInfo(pose_model.cfg.data['test'].get(\n","      'dataset_info', None))\n","    \n","\n","def vis_top_down(pose_model, dataset_info, person_result, image_path):\n","  # # test a single image, with a list of bboxes.\n","  pose_results = get_pose(\n","      pose_model, dataset_info, person_result, image_path)\n","  \n","  # show the results\n","  return vis_pose_result(\n","      pose_model, image_path, pose_results, dataset_info=dataset_info)\n","\n","def get_pose(pose_model, dataset_info, person_result, image_path):\n","  # # test a single image, with a list of bboxes.\n","  pose_results, _ = inference_top_down_pose_model(\n","      pose_model,\n","      image_path,\n","      person_result,\n","      format='xyxy',\n","      dataset_info=dataset_info)\n","  return pose_results"],"metadata":{"id":"d5fKfc8jFhtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"ZeJF-8MVKbJ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655193481770,"user_tz":-120,"elapsed":17151,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"}},"outputId":"32b18803-8029-4f84-d96c-e4b38aeabe26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/AdvancedComputerVision/annotation/annotation_image_info.json') as f:\n","  annotations = json.load(f)\n","\n","pose_model = init_model()\n","dataset_info = init_dataset_info(pose_model)"],"metadata":{"id":"WiD4bzZN6HVX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655193484386,"user_tz":-120,"elapsed":2623,"user":{"displayName":"Cristian Verdecchia","userId":"07676699127951801988"}},"outputId":"b709aabc-5c09-48f2-8e45-d0d220750b82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use load_from_local loader\n"]}]},{"cell_type":"code","source":["!mkdir images_with_detection"],"metadata":{"id":"RS1cgdfkf2sc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageDetections():\n","  def __init__(self, image_id):\n","    self.image_id = image_id  \n","    self.people_poses = []\n","\n","  def add_pose(self, pose):\n","    self.people_poses.append(pose)\n","\n","  def toJSON(self):\n","    return json.dumps(self, default=lambda o: o.__dict__,\n","                     sort_keys=True, indent=4)\n","    \n","class PersonDetection():\n","  def __init__(self, left_eye, right_eye, left_ear, right_ear,  left_shoulder, \n","               right_shoulder, left_elbow, right_elbow, left_wrist, right_wrist, \n","               left_hip, right_hip, left_knee, right_knee, left_ankle, right_ankle):\n","\n","    self.left_eye = left_eye.tolist()\n","    self.right_eye = right_eye.tolist()\n","    self.left_ear = left_ear.tolist()\n","    self.right_ear = right_ear.tolist()\n","    self.left_shoulder = left_shoulder.tolist()\n","    self.right_shoulder = right_shoulder.tolist()\n","    self.left_elbow = left_elbow.tolist()\n","    self.right_elbow = right_elbow.tolist()\n","    self.left_wrist = left_wrist.tolist()\n","    self.right_wrist = right_wrist.tolist()\n","    self.left_hip = left_hip.tolist()\n","    self.right_hip = right_hip.tolist()\n","    self.left_knee = left_knee.tolist()\n","    self.right_knee = right_knee.tolist()\n","    self.left_ankle = left_ankle.tolist()\n","    self.right_ankle = right_ankle.tolist()\n","\n","  # def __str__(self):\n","  #   return    \"\\nleft_eye:\\t \" + str(self.left_eye) + \\\n","  #                 \"\\nright_eye:\\t \" + str(self.right_eye) + \\\n","  #                 \"\\nleft_ear:\\t \" + str(self.left_ear) + \\\n","  #                 \"\\nright_ear:\\t \" + str(self.right_eay) + \\\n","  #                 \"\\nleft_shoulder:\\t \" + str(self.left_shoulder) + \\\n","  #                 \"\\nright_shoulder:\\t \" + str(self.right_shoulder) + \\\n","  #                 \"\\nleft_elbow:\\t \" + str(self.left_elbow) + \\\n","  #                 \"\\nright_elbow:\\t \" + str(self.right_elbow) + \\\n","  #                 \"\\nleft_wrist:\\t \" + str(self.left_wrist) + \\\n","  #                 \"\\nright_wrist:\\t \" + str(self.right_wrist) + \\\n","  #                 \"\\nleft_hip:\\t \" + str(self.left_hip) + \\\n","  #                 \"\\nright_hip:\\t \" + str(self.right_hip) + \\\n","  #                 \"\\nleft_knee:\\t \" + str(self.left_knee) + \\\n","  #                 \"\\nright_knee:\\t \" + str(self.right_knee) + \\\n","  #                 \"\\nleft_ankle:\\t \" + str(self.left_ankle) + \\\n","  #                 \"\\nright_ankle:\\t \" + str(self.right_ankle)"],"metadata":{"id":"1rMlvMA9h_n5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creates a new detection object (check the config for the meaning of the indexes)\n","def create_detection(detection):\n","  return PersonDetection(\n","      detection[0],\n","      detection[1],\n","      detection[2],\n","      detection[3],\n","      detection[4],\n","      detection[5],\n","      detection[6],\n","      detection[7],\n","      detection[8],\n","      detection[9],\n","      detection[10],\n","      detection[11],\n","      detection[12],\n","      detection[13],\n","      detection[14],\n","      detection[15]\n","      )"],"metadata":{"id":"oTZ-0Xj9yUJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Configuration for the joints that we use { display-mode: \"form\" }\n","# Copied from https://github.com/ViTAE-Transformer/ViTPose/blob/33f49cf1f24077ec175cef0990bd25d2746c4898/configs/_base_/datasets/coco_wholebody.py\n","# It contains the indexes for the configuration for the joints\n","# For example: the left eye can be saved with the id=1 and this configuration\n","# can be used to understand what the id is referring to\n","dataset_info = dict(\n","    keypoint_info={\n","        0:\n","        dict(name='nose', id=0, color=[51, 153, 255], type='upper', swap=''),\n","        1:\n","        dict(\n","            name='left_eye',\n","            id=1,\n","            color=[51, 153, 255],\n","            type='upper',\n","            swap='right_eye'),\n","        2:\n","        dict(\n","            name='right_eye',\n","            id=2,\n","            color=[51, 153, 255],\n","            type='upper',\n","            swap='left_eye'),\n","        3:\n","        dict(\n","            name='left_ear',\n","            id=3,\n","            color=[51, 153, 255],\n","            type='upper',\n","            swap='right_ear'),\n","        4:\n","        dict(\n","            name='right_ear',\n","            id=4,\n","            color=[51, 153, 255],\n","            type='upper',\n","            swap='left_ear'),\n","        5:\n","        dict(\n","            name='left_shoulder',\n","            id=5,\n","            color=[0, 255, 0],\n","            type='upper',\n","            swap='right_shoulder'),\n","        6:\n","        dict(\n","            name='right_shoulder',\n","            id=6,\n","            color=[255, 128, 0],\n","            type='upper',\n","            swap='left_shoulder'),\n","        7:\n","        dict(\n","            name='left_elbow',\n","            id=7,\n","            color=[0, 255, 0],\n","            type='upper',\n","            swap='right_elbow'),\n","        8:\n","        dict(\n","            name='right_elbow',\n","            id=8,\n","            color=[255, 128, 0],\n","            type='upper',\n","            swap='left_elbow'),\n","        9:\n","        dict(\n","            name='left_wrist',\n","            id=9,\n","            color=[0, 255, 0],\n","            type='upper',\n","            swap='right_wrist'),\n","        10:\n","        dict(\n","            name='right_wrist',\n","            id=10,\n","            color=[255, 128, 0],\n","            type='upper',\n","            swap='left_wrist'),\n","        11:\n","        dict(\n","            name='left_hip',\n","            id=11,\n","            color=[0, 255, 0],\n","            type='lower',\n","            swap='right_hip'),\n","        12:\n","        dict(\n","            name='right_hip',\n","            id=12,\n","            color=[255, 128, 0],\n","            type='lower',\n","            swap='left_hip'),\n","        13:\n","        dict(\n","            name='left_knee',\n","            id=13,\n","            color=[0, 255, 0],\n","            type='lower',\n","            swap='right_knee'),\n","        14:\n","        dict(\n","            name='right_knee',\n","            id=14,\n","            color=[255, 128, 0],\n","            type='lower',\n","            swap='left_knee'),\n","        15:\n","        dict(\n","            name='left_ankle',\n","            id=15,\n","            color=[0, 255, 0],\n","            type='lower',\n","            swap='right_ankle'),\n","        16:\n","        dict(\n","            name='right_ankle',\n","            id=16,\n","            color=[255, 128, 0],\n","            type='lower',\n","            swap='left_ankle'),\n","    })"],"metadata":{"id":"aXnGesvum1iW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saves images with the body pose detection\n","# def save_body_pose_images():\n","#   directory = '/content/drive/MyDrive/AdvancedComputerVision/image/'\n","#   boxes = []\n","#   cont = 0\n","\n","#   for image_name in sorted(os.listdir(directory)):\n","#     print(int(image_name[:-4]))\n","#     for box in annotations[int(image_name[:-4])]['bbox']:\n","#       boxes.append({'bbox': box})\n","#       cont = cont + 1\n","  \n","#   figure = vis_top_down(pose_model, dataset_info, boxes, directory+image_name)\n","#   im = Image.fromarray(figure)\n","#   im.save(\"images_with_detection/\"+image_name)\n","#   boxes = []"],"metadata":{"id":"t5HBax49K3sI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Estimates poses and creates Detection objects for each pose\n","def detect_poses():\n","  directory = '/content/drive/MyDrive/AdvancedComputerVision/image/'\n","  boxes = []\n","  list_poses = []\n","  image_detection_list = []\n","  cont = 0\n","\n","  # Loop images in the folder\n","  for image_name in sorted(os.listdir(directory)):\n","    image_id = int(image_name[:-4])\n","    print(image_id+1)\n","\n","    image_detection = ImageDetections(image_id)\n","    # Create an object with the file and the list of poses\n","    image_detection_list.append(image_detection)\n","\n","    # Obtain annotations for the given image\n","    for box in annotations[image_id]['bbox']:\n","      boxes.append({'bbox': box})\n","\n","    # Obtain poses for each bbox\n","    obtained_poses = get_pose(pose_model, dataset_info, boxes, directory+image_name)\n","    \n","    # For each pose create a detection object\n","    for single_pose in obtained_poses:\n","      image_detection.add_pose(\n","          create_detection(\n","              single_pose['keypoints'][:,:-1]))\n","    \n","    boxes = []\n","    # Break the loop (just for testing)\n","    cont = cont + 1\n","    if cont==10:\n","      return image_detection_list"],"metadata":{"id":"rOf6pOu-wQBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_detection_list = []\n","directory = '/content/drive/MyDrive/AdvancedComputerVision/image/'\n","\n","# Estimates poses and creates Detection objects for each pose\n","def detect_poses(image_name):\n","  boxes = []\n","  list_poses = []\n","  cont = 0\n","\n","  image_id = int(image_name[:-4])\n","  print(image_id+1)\n","\n","  image_detection = ImageDetections(image_id)\n","  # Create an object with the file and the list of poses\n","  image_detection_list.append(image_detection)\n","\n","  # Obtain annotations for the given image\n","  for box in annotations[image_id]['bbox']:\n","    boxes.append({'bbox': box})\n","\n","  # Obtain poses for each bbox\n","  obtained_poses = get_pose(pose_model, dataset_info, boxes, directory+image_name)\n","  \n","  # For each pose create a detection object\n","  for single_pose in obtained_poses:\n","    image_detection.add_pose(\n","        create_detection(\n","            single_pose['keypoints'][:,:-1]))\n"],"metadata":{"id":"oEPEUAvf_FgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Parallelise the process, so to compute the same for multiple images\n","# from multiprocessing import Pool\n","\n","# directory = '/content/drive/MyDrive/AdvancedComputerVision/image/'\n","# list_images = sorted(os.listdir(directory))\n","\n","# pool = Pool(os.cpu_count())\n","# pool.map(detect_poses, list_images)\n","# pool.close()"],"metadata":{"id":"JZ3iQZVs-58A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def test_method():\n","#   boxes = []\n","  \n","#   for box in annotations[1]['bbox']:\n","#     boxes.append({'bbox': box})\n","  \n","#   return get_pose(pose_model, dataset_info, boxes, '00001.jpg')\n","\n","# image_1 = test_method()"],"metadata":{"id":"Q68Iaj68ze3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The third column (index 2) represents the score (a sort of accuracy)\n","# print(image_1[0])\n","# print(image_1[0]['keypoints'])\n","# print(image_1[0]['keypoints'][:,:-1][0])"],"metadata":{"id":"g10VikFS0EqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list_poses = detect_poses()\n","\n","# Serialization\n","with open(\"serialized_poses.json\", \"w\") as outfile:\n","    json.dump(list_poses, outfile, default=lambda o: o.__dict__ )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nur9yIvSEMvj","executionInfo":{"status":"ok","timestamp":1655192263349,"user_tz":-120,"elapsed":46087,"user":{"displayName":"Sara-Jane Bittner","userId":"00590223055988168523"}},"outputId":"41f5a819-cb8b-48b5-fbf7-f9800ba27de6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n"]}]},{"cell_type":"code","source":["#list_poses[0].people_poses[0].__dict__"],"metadata":{"id":"Mjbi92h_l3c6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#list_poses[0].toJSON()"],"metadata":{"id":"-UGHv9LJjKbz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#json.dumps(list_poses, default=lambda o: o.__dict__)"],"metadata":{"id":"4DG_z1ZmjTPE"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"vitpose_bodyposes.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}