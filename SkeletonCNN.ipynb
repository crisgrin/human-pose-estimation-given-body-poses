{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0\n",
    "# !pip install mmcv-full==1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
    "\n",
    "# !pip install openmim\n",
    "# !mim install mmcv-full\n",
    "# !git clone https://github.com/open-mmlab/mmpose.git\n",
    "# !cd mmpose && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install tensorflow-macos\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# for modeling\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Restart the environment after the installation and before the import\n",
    "from mmpose.core.visualization import imshow_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"five_hundred\"\n",
    "GRAY_IMAGE = False\n",
    "RESIZE_320 = True\n",
    "\n",
    "IMAGE_RESOLUTION = \"320_320\" if RESIZE_320 else \"640_640\"\n",
    "GRAY_SCALE = '1' if GRAY_IMAGE else '3'\n",
    "\n",
    "FOLDER_PATH = '../posture_data/'+DATASET_SIZE+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton settings and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linking between the joints of the skeleton\n",
    "skeleton = [[0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 5], [4, 6], [5, 7], [6,8], [7, 9], \n",
    "            [4, 10], [5, 11], [10,11], [10, 12], [11, 13], [12, 14], [13, 15]]\n",
    "\n",
    "# Palette of colors to choose from\n",
    "palette = np.array([[255, 128, 0], [255, 153, 51], [255, 178, 102],\n",
    "                    [230, 230, 0], [255, 153, 255], [153, 204, 255],\n",
    "                    [255, 102, 255], [255, 51, 255], [102, 178, 255],\n",
    "                    [51, 153, 255], [255, 153, 153], [255, 102, 102],\n",
    "                    [255, 51, 51], [153, 255, 153], [102, 255, 102],\n",
    "                    [51, 255, 51], [0, 255, 0], [0, 0, 255],\n",
    "                    [255, 0, 0], [255, 255, 255]])\n",
    "\n",
    "# pose_kpt_color = [None] + [(127, 127, 127)] * (len(kpts[1]) - 1)\n",
    "# Colors for the joints\n",
    "pose_kpt_color = palette[[\n",
    "            0, 0, 0, 0, 7, 7, 9, 9, 9, 9, \n",
    "            7, 7, 16, 16, 16, 16\n",
    "        ]]    \n",
    "\n",
    "# Colors for the linkings\n",
    "pose_link_color = palette[[\n",
    "                0, 0, 0, 0, 0, 7, 9, 9, 9, 9,\n",
    "                7, 7, 7, 16, 16, 16, 16 \n",
    "            ]]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw lines given images, key points and the linkings between the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_joint_line(img, kpts):\n",
    "    for person in kpts:\n",
    "        for joint in person:\n",
    "            joint.append(1)\n",
    "\n",
    "    # Draw the linkings between joints and return the image\n",
    "    return imshow_keypoints(\n",
    "        img,\n",
    "        kpts,\n",
    "        skeleton = skeleton,\n",
    "        pose_kpt_color = pose_kpt_color,\n",
    "        pose_link_color = pose_link_color,\n",
    "        show_keypoint_weight=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw skeletons for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_bb.json\n",
      "|██████▌⚠︎                                | (!) 81/500 [16%] in 2.7s (30.28/s)                                           \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ppl_joints\u001b[39m.\u001b[39mappend(\u001b[39mlist\u001b[39m(image[\u001b[39m'\u001b[39m\u001b[39mperson_2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m zeros_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m640\u001b[39m, \u001b[39m640\u001b[39m, \u001b[39m3\u001b[39m), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m skeleton_on_image \u001b[39m=\u001b[39m draw_joint_line(zeros_image, ppl_joints)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m RESIZE_320:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   skeleton_on_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(skeleton_on_image, (\u001b[39m320\u001b[39m, \u001b[39m320\u001b[39m))\n",
      "\u001b[1;32m/Users/cristianverdecchia/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb Cell 14\u001b[0m in \u001b[0;36mdraw_joint_line\u001b[0;34m(img, kpts)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         joint\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Draw the linkings between joints and return the image\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m imshow_keypoints(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     img,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     kpts,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     skeleton \u001b[39m=\u001b[39;49m skeleton,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     pose_kpt_color \u001b[39m=\u001b[39;49m pose_kpt_color,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     pose_link_color \u001b[39m=\u001b[39;49m pose_link_color,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     show_keypoint_weight\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cristianverdecchia/Documents/University/Master/Advanced%20Computer%20Vision%20%26%20Image%20Processing/human_pose_estimation/SJ-CV/SkeletonCNN.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/mmcv/utils/misc.py:330\u001b[0m, in \u001b[0;36mdeprecated_api_warning.<locals>.api_warning_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m             kwargs[dst_arg_name] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(src_arg_name)\n\u001b[1;32m    329\u001b[0m \u001b[39m# apply converted arguments to the decorated method\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m output \u001b[39m=\u001b[39m old_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    331\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/University/Master/Advanced Computer Vision & Image Processing/human_pose_estimation/SJ-CV/ViTPose/mmpose/core/visualization/image.py:192\u001b[0m, in \u001b[0;36mimshow_keypoints\u001b[0;34m(img, pose_result, skeleton, kpt_score_thr, pose_kpt_color, pose_link_color, radius, thickness, show_keypoint_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m     cv2\u001b[39m.\u001b[39mfillConvexPoly(img_copy, polygon, color)\n\u001b[1;32m    190\u001b[0m     transparency \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m0\u001b[39m, \u001b[39mmin\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (kpts[sk[\u001b[39m0\u001b[39m], \u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m kpts[sk[\u001b[39m1\u001b[39m], \u001b[39m2\u001b[39m])))\n\u001b[0;32m--> 192\u001b[0m     cv2\u001b[39m.\u001b[39;49maddWeighted(\n\u001b[1;32m    193\u001b[0m         img_copy,\n\u001b[1;32m    194\u001b[0m         transparency,\n\u001b[1;32m    195\u001b[0m         img,\n\u001b[1;32m    196\u001b[0m         \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m transparency,\n\u001b[1;32m    197\u001b[0m         \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    198\u001b[0m         dst\u001b[39m=\u001b[39;49mimg)\n\u001b[1;32m    199\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     cv2\u001b[39m.\u001b[39mline(img, pos1, pos2, color, thickness\u001b[39m=\u001b[39mthickness)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from alive_progress import alive_bar\n",
    "\n",
    "full_images = []\n",
    "\n",
    "# Loop through the annotation files\n",
    "for file_label in os.listdir(FOLDER_PATH):\n",
    "  # Read the file content\n",
    "  with open(FOLDER_PATH + file_label) as f:\n",
    "    file_annotations = json.load(f)\n",
    "  print(file_label)\n",
    "  \n",
    "  take_half = True\n",
    "  with alive_bar(int(len(file_annotations.values())), force_tty = True) as bar:\n",
    "    # Loop through the images and save the data\n",
    "    for image in file_annotations.values():\n",
    "      # if take_half:\n",
    "      ppl_joints = [list(image['person_1'].values())]\n",
    "      ppl_joints.append(list(image['person_2'].values()))\n",
    "      \n",
    "      zeros_image = np.zeros((640, 640, 3), dtype=\"float32\")\n",
    "      skeleton_on_image = draw_joint_line(zeros_image, ppl_joints)\n",
    "      if RESIZE_320:\n",
    "        skeleton_on_image = cv2.resize(skeleton_on_image, (320, 320))\n",
    "      if GRAY_IMAGE:\n",
    "        skeleton_on_image = cv2.cvtColor(skeleton_on_image, cv2.COLOR_RGB2GRAY)\n",
    "                  \n",
    "      # Creates a list of images with the annotations\n",
    "      full_images.append((\n",
    "          skeleton_on_image, \n",
    "          int(file_label[0])\n",
    "          ))  \n",
    "      bar()\n",
    "      \n",
    "      #   take_half = False\n",
    "      # else:\n",
    "      #   take_half = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the images with the poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristianverdecchia/miniconda3/envs/skeletoncnn/lib/python3.9/site-packages/numpy/lib/npyio.py:501: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "with open('../posture_data/images_skeletons/images_with_poses_{}_{}_{}.npy'.format(DATASET_SIZE, IMAGE_RESOLUTION, GRAY_SCALE), 'wb') as f:\n",
    "    # np.save(f, np.array(full_images, dtype=object)) \n",
    "    np.save(f, full_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the data, read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../posture_data/images_skeletons/images_with_poses_{}_{}_{}.npy'.format(DATASET_SIZE, IMAGE_RESOLUTION, GRAY_SCALE), 'rb') as f:\n",
    "    images_with_poses = np.load(f, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the images for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(images_with_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for image in images_with_poses:\n",
    "    x.append(image[0])\n",
    "    y.append(image[1])\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train /= 255.0\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2850"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRAY_IMAGE:\n",
    "    x_train_unidimensional = x_train.reshape(-1, 320, 320, 1)\n",
    "    x_test_unidimensional = x_test.reshape(-1, 320, 320, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_train.reshape(-1, 1))\n",
    "y_train = encoder.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet-18\n",
    "Reference:\n",
    "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
    "Surpassing human-level performance on imagenet classification. In\n",
    "ICCV, 2015.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(Model):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(16, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(16, down_sample=True)\n",
    "        self.res_1_2 = ResnetBlock(16)\n",
    "        self.res_2_1 = ResnetBlock(32, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(32)\n",
    "        self.res_3_1 = ResnetBlock(64, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(64)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disables GPU\n",
    "# !python -m pip uninstall tensorflow-metal -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method ResnetBlock.call of <__main__.ResnetBlock object at 0x176e8aeb0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ResnetBlock.call of <__main__.ResnetBlock object at 0x176e8aeb0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"res_net18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  2368      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  multiple                 64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block (ResnetBlock)  multiple                  5104      \n",
      "                                                                 \n",
      " resnet_block_1 (ResnetBlock  multiple                 4768      \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_2 (ResnetBlock  multiple                 14816     \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_3 (ResnetBlock  multiple                 18752     \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_4 (ResnetBlock  multiple                 58304     \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_5 (ResnetBlock  multiple                 74368     \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d (G  multiple                 0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,934\n",
      "Trainable params: 177,782\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18(6)\n",
    "model.build(input_shape = (None, 320, 320, 3))\n",
    "model.compile(optimizer = \"adam\",\n",
    "            #   loss='categorical_crossentropy', \n",
    "                loss=keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[\"accuracy\"]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 13:10:20.239085: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 30s 252ms/step - loss: 1.8247 - accuracy: 0.1754 - val_loss: 1.7878 - val_accuracy: 0.2267\n",
      "Epoch 2/50\n",
      "114/114 [==============================] - 30s 266ms/step - loss: 1.7889 - accuracy: 0.1930 - val_loss: 1.8049 - val_accuracy: 0.1667\n",
      "Epoch 3/50\n",
      "114/114 [==============================] - 31s 270ms/step - loss: 1.7440 - accuracy: 0.2460 - val_loss: 1.9942 - val_accuracy: 0.1933\n",
      "Epoch 4/50\n",
      "114/114 [==============================] - 32s 280ms/step - loss: 1.6836 - accuracy: 0.2961 - val_loss: 2.0371 - val_accuracy: 0.1867\n",
      "Epoch 5/50\n",
      "114/114 [==============================] - 32s 277ms/step - loss: 1.5849 - accuracy: 0.3432 - val_loss: 2.8193 - val_accuracy: 0.1533\n",
      "Epoch 6/50\n",
      "114/114 [==============================] - 30s 265ms/step - loss: 1.4787 - accuracy: 0.4218 - val_loss: 3.4716 - val_accuracy: 0.1800\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "STEPS = len(x_train) / 256\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "\n",
    "x_to_train = []\n",
    "x_to_test = []\n",
    "\n",
    "if GRAY_IMAGE:\n",
    "    x_to_train = x_train_unidimensional\n",
    "    x_to_test = x_test_unidimensional\n",
    "else:\n",
    "    x_to_train = x_train\n",
    "    x_to_test = x_test\n",
    "    \n",
    "history = model.fit(x_to_train, y_train, batch_size = 25, epochs=50, validation_data=(x_to_test, y_test), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 67ms/step - loss: 1.7878 - accuracy: 0.2267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.787793755531311, 0.2266666740179062]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_to_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn/five_hundred_640_640_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn/five_hundred_640_640_1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/cnn/{}_{}_{}\".format(DATASET_SIZE, IMAGE_RESOLUTION, GRAY_SCALE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "# learning curve\n",
    "# accuracy\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "\n",
    "# loss\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# plot\n",
    "# \"r\" is for \"solid red line\"\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 185ms/step\n",
      "Confusion matrix\n",
      "[[22  3  0  1  0  0]\n",
      " [ 1 23  0  1  2  0]\n",
      " [ 3 11  5  3  1  0]\n",
      " [ 2  3  1 21  2  0]\n",
      " [ 0 12  0  1  9  0]\n",
      " [ 2  4  0  1  3 13]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.85      0.79        26\n",
      "           1       0.41      0.85      0.55        27\n",
      "           2       0.83      0.22      0.34        23\n",
      "           3       0.75      0.72      0.74        29\n",
      "           4       0.53      0.41      0.46        22\n",
      "           5       1.00      0.57      0.72        23\n",
      "\n",
      "    accuracy                           0.62       150\n",
      "   macro avg       0.71      0.60      0.60       150\n",
      "weighted avg       0.70      0.62      0.61       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "preds = model.predict(x_to_test) \n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), preds.argmax(axis=1))\n",
    "print(\"Confusion matrix\")\n",
    "print(matrix)\n",
    "print('')\n",
    "# more detail on how well things were predicted\n",
    "print(classification_report(y_test.argmax(axis=1), preds.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('skeletoncnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2dbd656668c842d22c7cc804324ef75aa425d033a96c56443a55074cc19dd74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
